# Podcast

<div style="position: relative; text-align: center;">
  <img src="podcast_background.png" alt="Background" style="width: 100%; max-width: 800px; border-radius: 10px;">
  <div style="position: absolute; bottom: 20px; left: 50%; transform: translateX(-50%);">
    <audio controls>
      <source src="audio_podcast.mp3" type="audio/mpeg">
      Your browser does not support the audio element.
    </audio>
  </div>
</div>

# Research Visual Flow Chart
![Flowchart]([flowchart.png])

# Confusion Matrix (Model's predictions vs. Real labels) and Feature Importance for the ML (tabular data) XGBoost model


The first image shows each of the model's predictions and the corresponding truth values. Notice that the model correctly predicted a large majority of patients that were actually over 8 hours, but also predicted the large majority of patients under 8 hours to be over 8 hours. 




# My project explained simply:

I got data from a large dataset of patients, found a cohort with chest pain presenting patients and collected their ECGs from their ED stay. I filtered the data and the reports generated by the machine that conducted the ECGs. I ran an ML XGBoost model on tabular features from the ECG machine reports as well as vitals and other non-ECG variables, which performed as shown above. I then wanted to see if a deep learning model built solely on the raw ECG waveforms (not tabular data) could perform better in predicting a patient's length of stay. I trained the DL model to recognize ECG patterns, then tuned it specifically to predict length of stay based on raw waveforms. In the end, it performed poorly, leading me to conclude that ECGs alone probably aren't the best way to predict how long a chest pain-presenting patient will stay in the ED for. 
